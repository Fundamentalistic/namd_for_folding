%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
%              (C) Copyright 1995 The Board of Trustees of the             %
%                          University of Illinois                          %
%                           All Rights Reserved                            %
%								  	   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Running \NAMD}
\label{section:run}

NAMD runs on a variety of serial and parallel platforms.  While it is
trivial to launch a serial program, a parallel program depends on a
platform-specific library such as MPI to launch copies of itself on
other nodes and to provide access to a high performance network such
as Myrinet or InfiniBand if one is available.

For typical workstations (Windows, Linux, Mac OS X, or other Unix)
with only ethernet networking (hopefully gigabit), NAMD uses the
Charm++ native communications layer and the program charmrun to launch
namd2 processes for parallel runs (either exclusively on the local
machine with the ++local option or on other hosts as specified by a
nodelist file).  The namd2 binaries for these platforms can also be
run directly (known as standalone mode) for single process runs.

Charm++ provides a special ibverbs network layer that uses InfiniBand
networks directly through the OpenFabrics OFED ibverbs library.  This
avoids efficiency and portability issues associated with MPI.  Look for
pre-built ibverbs NAMD binaries or specify ibverbs when building Charm++.
All runs must be launched with charmrun (standalone mode does not work).

For workstation clusters and other massively parallel machines with
special high-performance networking, NAMD uses the system-provided
MPI library (with a few exceptions) and standard system tools such as
mpirun are used to launch jobs.  Since MPI libraries are very often
incompatible between versions, you will likely need to recompile NAMD
and its underlying Charm++ libraries to use these machines in parallel
(the provided non-MPI binaries should still work for serial runs.)
The provided charmrun program for these platforms is only a script
that attempts to translate charmrun options into mpirun options, but
due to the diversity of MPI libraries it often fails to work.

\subsection{Individual Windows, Linux, Mac OS X, or Other Unix Workstations}

Individual workstations use the same version of NAMD as workstation
networks, but running NAMD is much easier.  If your machine has only
one processor core you can run the any non-MPI namd2 binary directly:

\begin{verbatim}
  namd2 <configfile>
\end{verbatim}

For multicore workstations, Windows and Mac OX X (Intel) released binaries
are based on ``multicore'' builds of Charm++ that can run multiple threads.
These multicore builds lack a network layer, so they can only be used on a
single machine.
The Solaris (Sparc and x86-64) released binaries are based on ``smp''
builds of Charm++ that can be used with multiple threads on either a
single machine like a multicore build, or across a network.
For best performance use one thread per processor with the +p option:

\begin{verbatim}
  namd2 +p<procs> <configfile>
\end{verbatim}

For other multiprocessor workstations the included charmrun program is
needed to run multiple namd2 processes.  The ++local option is also
required to specify that only the local machine is being used:

\begin{verbatim}
  charmrun namd2 ++local +p<procs> <configfile>
\end{verbatim}

You may need to specify the full path to the namd2 binary.

\subsection{Linux or Other Unix Workstation Networks}

The same binaries used for individual workstations as described above
(other than pure ``multicore'' builds and MPI builds)
can be used with charmrun to run in parallel on a workstation network.
The only difference is that you must provide a ``nodelist'' file listing
the machines where namd2 processes should run, for example:

\begin{verbatim}
  group main
  host brutus
  host romeo
\end{verbatim}

The ``group main'' line defines the default machine list.  Hosts brutus
and romeo are the two machines on which to run the simulation.  Note
that charmrun may run on one of those machines, or charmrun may run
on a third machine.  All machines used for a simulation must be of the
same type and have access to the same namd2 binary.

By default, the ``rsh'' command is used to start namd2
on each node specified in the nodelist file.  You can change this via
the CONV\_RSH environment variable, i.e., to use ssh instead of rsh run
``setenv CONV\_RSH ssh'' or add it to your login or batch script.  You
must be able to connect to each node via rsh/ssh without typing your
password; this can be accomplished via a .rhosts files in your home
directory, by an /etc/hosts.equiv file installed by your sysadmin, or
by a .ssh/authorized\_keys file in your home directory.  You should
confirm that you can run ``ssh hostname pwd'' (or ``rsh hostname pwd'')
without typing a password before running NAMD.  Contact your local
sysadmin if you have difficulty setting this up.  If you are unable to
use rsh or ssh, then add ``setenv CONV\_DAEMON'' to your script and run 
charmd (or charmd\_faceless, which produces a log file) on every node.

You should now be able to try running NAMD as:

\begin{verbatim}
  charmrun namd2 +p<procs> <configfile>
\end{verbatim}

If this fails or just hangs, try adding the ++verbose option to see
more details of the startup process.  You may need to specify the full
path to the namd2 binary.  Charmrun will start the number of processes
specified by the +p option, cycling through the hosts in the nodelist
file as many times as necessary.  You may list multiprocessor machines
multiple times in the nodelist file, once for each processor.

You may specify the nodelist file with the ``++nodelist'' option and the
group (which defaults to ``main'') with the ``++nodegroup'' option.  If
you do not use ``++nodelist'' charmrun will first look for ``nodelist''
in your current directory and then ``.nodelist'' in your home directory.

Some automounters use a temporary mount directory which is prepended
to the path returned by the pwd command.  To run on multiple machines
you must add a ``++pathfix'' option to your nodelist file.  For example:

\begin{verbatim}
  group main ++pathfix /tmp\_mnt /
  host alpha1
  host alpha2
\end{verbatim}

There are many other options to charmrun and for the nodelist file.
These are documented at in the Charm++ Installation and Usage Manual
available at http://charm.cs.uiuc.edu/manuals/ and a list of available
charmrun options is available by running charmrun without arguments.

If your workstation cluster is controlled by a queueing system you
will need build a nodelist file in your job script.  For example, if
your queueing system provides a HOST\_FILE environment variable:

\begin{verbatim}
  set NODES = `cat $HOST_FILE`
  set NODELIST = $TMPDIR/namd2.nodelist
  echo group main >! $NODELIST
  foreach node ( $nodes )
    echo host $node >> $NODELIST
  end
  @ NUMPROCS = 2 * $#NODES
  charmrun namd2 +p$NUMPROCS ++nodelist $NODELIST <configfile>
\end{verbatim}

Note that NUMPROCS is twice the number of nodes in this example.
This is the case for dual-processor machines.  For single-processor
machines you would not multiply \$\#NODES by two.

Note that these example scripts and the setenv command are for the csh
or tcsh shells.  They must be translated to work with sh or bash.

\subsection{Windows Workstation Networks}

These are no longer supported, but NAMD has been reported to compile
on Windows HPC Server.

\subsection{SGI Altix}

Be sure that the {\tt MPI\_DSM\_DISTRIBUTE} environment variable is set, then
use the Linux-Itanium-MPI-Altix version of NAMD along with the system mpirun:

\begin{verbatim}
mpirun -np <procs> <configfile>
\end{verbatim}

\subsection{IBM POWER Clusters}

Run the MPI version of NAMD as you would any POE program.  The options
and environment variables for poe are various and arcane, so you should
consult your local documentation for recommended settings.  As an
example, to run on Blue Horizon one would specify:

\begin{verbatim}
  poe namd2 <configfile> -nodes <procs/8> -tasks_per_node 8
\end{verbatim}

\subsection{CUDA GPU Acceleration}

NAMD only uses the GPU for nonbonded force evaluation.  Energy evaluation
is done on the CPU.  To benefit from GPU acceleration you should set
outputEnergies to 100 or higher in the simulation config file.  Some
features are unavailable in CUDA builds, including alchemical free
energy perturbation and non-isotropic (flexible cell) pressure control
(the scalar pressure is correct but the pressure tensor is incorrect).

As this is a new feature you are encouraged to test all simulations
before beginning production runs.  Forces evaluated on the GPU differ
slightly from a CPU-only calculation, an effect more visible in reported
scalar pressure values than in energies.

To benefit from GPU acceleration you will need a CUDA build of NAMD
and a recent high-end NVIDIA video card.  CUDA builds will not function
without a CUDA-capable GPU.  You will also need to be running the
NVIDIA Linux driver version 195.17 or newer (NAMD 2.7b3 released binaries
are built with CUDA 2.3, but can be built with 3.0 or 3.1 as well).

Finally, the libcudart.so.2 included with the binary (the one copied from
the version of CUDA it was built with) must be in a directory in your
LD\_LIBRARY\_PATH before any other libcudart.so libraries.  For example:

\begin{verbatim}
  setenv LD_LIBRARY_PATH ".:$LD_LIBRARY_PATH"
  (or LD_LIBRARY_PATH=".:$LD_LIBRARY_PATH"; export LD_LIBRARY_PATH)
  ./namd2 +idlepoll <configfile>
  ./charmrun ++local +p4 ./namd2 +idlepoll <configfile>
\end{verbatim}

When running CUDA NAMD always add +idlepoll to the command line.  This
is needed to poll the GPU for results rather than sleeping while idle.

Each namd2 process can use only one GPU.  Therefore you will need to run
at least one process for each GPU you want to use.  Multiple processes
can share a single GPU, usually with an increase in performance.  NAMD
will automatically distribute processes equally among the GPUs on a node.
Specific GPU device IDs can be requested via the ++devices argument on
the namd2 command line, for example:

\begin{verbatim}
  ./charmrun ++local +p4 ./namd2 +idlepoll +devices 0,2 <configfile>
\end{verbatim}

Devices are selected cyclically from those available, so in the above
example processes 0 and 2 will share device 0 and processes 1 and 3 will
share device 2.  One could also specify +devices 0,0,2,2 to cause device
0 to be shared by processes 0 and 1, etc.  GPUs with two or fewer
multiprocessors are ignored unless specifically requested with ++devices.

While charmrun with ++local will preserve LD\_LIBRARY\_PATH, normal
charmrun does not.  You can use charmrun ++runscript to add the namd2
directory to LD\_LIBRARY\_PATH with the following executable runscript:

\begin{verbatim}
  #!/bin/csh
  setenv LD_LIBRARY_PATH "${1:h}:$LD_LIBRARY_PATH"
  $*
\end{verbatim}

For example:

\begin{verbatim}
  ./charmrun ++runscript ./runscript +p8 ./namd2 +idlepoll <configfile>
\end{verbatim}

An InfiniBand network is highly recommended when running CUDA-accelerated
NAMD across multiple nodes.  You will need either an ibverbs NAMD binary
(available for download) or an MPI NAMD binary (must build Charm++ and
NAMD as described above) to make use of the InfiniBand network.

The CUDA (NVIDIA's graphics processor programming platform) code in
NAMD is completely self-contained and does not use any of the CUDA
support features in Charm++.  When building NAMD with CUDA support
you should use the same Charm++ you would use for a non-CUDA build.
Do NOT add the cuda option to the Charm++ build command line.  The
only changes to the build process needed are to add --with-cuda and
possibly --cuda-prefix ... to the NAMD config command line.

\subsection{Memory Usage}

NAMD has traditionally used less than 100MB of memory even for systems
of 100,000 atoms.  With the reintroduction of pairlists in NAMD 2.5,
however, memory usage for a 100,000 atom system with a 12A cutoff can
approach 300MB, and will grow with the cube of the cutoff.  This extra
memory is distributed across processors during a parallel run, but a
single workstation may run out of physical memory with a large system.

To avoid this, NAMD now provides a pairlistMinProcs config file option
that specifies the minimum number of processors that a run must use
before pairlists will be enabled (on fewer processors small local
pairlists are generated and recycled rather than being saved, the
default is ``pairlistMinProcs 1'').  This is a per-simulation rather than
a compile time option because memory usage is molecule-dependent.

Additional information on reducing memory usage may be found at
http://www.ks.uiuc.edu/Research/namd/wiki/index.cgi?NamdMemoryReduction

\subsection{Improving Parallel Scaling}

While NAMD is designed to be a scalable program, particularly for
simulations of 100,000 atoms or more, at some point adding additional
processors to a simulation will provide little or no extra performance.
If you are lucky enough to have access to a parallel machine you should
measure NAMD's parallel speedup for a variety of processor counts when
running your particular simulation.  The easiest and most accurate way
to do this is to look at the ``Benchmark time:'' lines that are printed
after 20 and 25 cycles (usually less than 500 steps).  You can monitor
performance during the entire simulation by adding ``outputTiming {\em steps}''
to your configuration file, but be careful to look at the ``wall time''
rather than ``CPU time'' fields on the ``TIMING:'' output lines produced.
For an external measure of performance, you should run simulations of
both 25 and 50 cycles (see the stepspercycle parameter) and base your
estimate on the additional time needed for the longer simulation in
order to exclude startup costs and allow for initial load balancing.

We provide both standard (UDP) and TCP based precompiled binaries
for Linux clusters.  We have observed that the TCP version is better
on our dual processor clusters with gigabit ethernet while the basic
UDP version is superior on our single processor fast ethernet cluster.
When using the UDP version with gigabit you can add the +giga option
to adjust several tuning parameters.  Additional performance may be
gained by building NAMD against an SMP version of Charm++ such as
net-linux-smp or net-linux-smp-icc.  This will use a communication
thread for each process to respond to network activity more rapidly.
For dual processor clusters we have found it that running two separate
processes per node, each with its own communication thread, is faster
than using the charmrun ++ppn option to run multiple worker threads.
However, we have observed that when running on a single hyperthreaded
processor (i.e., a newer Pentium 4) there is an additional 15\% boost
from running standalone with two threads (namd2 +p2) beyond running
two processors (charmrun namd2 ++local +p2).  For a cluster of single
processor hyperthreaded machines an SMP version should provide very
good scaling running one process per node since the communication
thread can run very efficiently on the second virtual processor.  We
are unable to ship an SMP build for Linux due to portability problems
with the Linux pthreads implementation needed by Charm++.

Extremely short cycle lengths (less than 10 steps) will also limit
parallel scaling, since the atom migration at the end of each cycle
sends many more messages than a normal force evaluation.  Increasing
pairlistdist from, e.g., cutoff + 1.5 to cutoff + 2.5, while also
doubling stepspercycle from 10 to 20, may increase parallel scaling,
but it is important to measure.  When increasing stepspercycle, also
try increasing pairlistspercycle by the same proportion.

NAMD should scale very well when the number of patches (multiply the
dimensions of the patch grid) is larger or rougly the same as the
number of processors.  If this is not the case, it may be possible
to improve scaling by adding ``twoAwayX yes'' to the config file,
which roughly doubles the number of patches.  (Similar options
twoAwayY and twoAwayZ also exist, and may be used in combination,
but this greatly increases the number of compute objects.  twoAwayX
has the unique advantage of also improving the scalability of PME.)
\index{twoAwayX} \index{twoAwayY} \index{twoAwayZ}

Additional performance tuning suggestions and options are described
at http://www.ks.uiuc.edu/Research/namd/wiki/?NamdPerformanceTuning

